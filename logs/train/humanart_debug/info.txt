[05/05 21:38:25.474]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 21:38:25.477]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --person_only
[05/05 21:38:25.482]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 21:38:25.482]: world size: 1
[05/05 21:38:25.483]: rank: 0
[05/05 21:38:25.483]: local_rank: None
[05/05 21:38:25.483]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=True, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 21:38:27.509]: number of params:47883345
[05/05 21:38:27.520]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 256,
  "transformer.decoder.class_embed.0.bias": 1,
  "transformer.decoder.class_embed.1.weight": 256,
  "transformer.decoder.class_embed.1.bias": 1,
  "transformer.decoder.class_embed.2.weight": 256,
  "transformer.decoder.class_embed.2.bias": 1,
  "transformer.decoder.class_embed.3.weight": 256,
  "transformer.decoder.class_embed.3.bias": 1,
  "transformer.decoder.class_embed.4.weight": 256,
  "transformer.decoder.class_embed.4.bias": 1,
  "transformer.decoder.class_embed.5.weight": 256,
  "transformer.decoder.class_embed.5.bias": 1,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 256,
  "transformer.enc_out_class_embed.bias": 1,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/05 21:38:28.053]: Ignore keys: []
[05/05 21:38:28.145]: _IncompatibleKeys(missing_keys=['transformer.decoder.gesture_embed.weight', 'transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])
[05/05 21:44:17.747]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 21:44:17.751]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --person_only
[05/05 21:44:17.759]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 21:44:17.759]: world size: 1
[05/05 21:44:17.760]: rank: 0
[05/05 21:44:17.760]: local_rank: None
[05/05 21:44:17.761]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=True, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 21:44:19.074]: number of params:47883345
[05/05 21:44:19.084]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 256,
  "transformer.decoder.class_embed.0.bias": 1,
  "transformer.decoder.class_embed.1.weight": 256,
  "transformer.decoder.class_embed.1.bias": 1,
  "transformer.decoder.class_embed.2.weight": 256,
  "transformer.decoder.class_embed.2.bias": 1,
  "transformer.decoder.class_embed.3.weight": 256,
  "transformer.decoder.class_embed.3.bias": 1,
  "transformer.decoder.class_embed.4.weight": 256,
  "transformer.decoder.class_embed.4.bias": 1,
  "transformer.decoder.class_embed.5.weight": 256,
  "transformer.decoder.class_embed.5.bias": 1,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 256,
  "transformer.enc_out_class_embed.bias": 1,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/05 21:44:19.560]: Ignore keys: []
[05/05 21:44:19.648]: _IncompatibleKeys(missing_keys=['transformer.decoder.gesture_embed.weight', 'transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])
[05/05 21:48:10.347]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 21:48:10.351]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed.
[05/05 21:48:10.358]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 21:48:10.358]: world size: 1
[05/05 21:48:10.358]: rank: 0
[05/05 21:48:10.359]: local_rank: None
[05/05 21:48:10.359]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 21:48:11.649]: number of params:47894139
[05/05 21:48:11.659]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 1792,
  "transformer.decoder.class_embed.0.bias": 7,
  "transformer.decoder.class_embed.1.weight": 1792,
  "transformer.decoder.class_embed.1.bias": 7,
  "transformer.decoder.class_embed.2.weight": 1792,
  "transformer.decoder.class_embed.2.bias": 7,
  "transformer.decoder.class_embed.3.weight": 1792,
  "transformer.decoder.class_embed.3.bias": 7,
  "transformer.decoder.class_embed.4.weight": 1792,
  "transformer.decoder.class_embed.4.bias": 7,
  "transformer.decoder.class_embed.5.weight": 1792,
  "transformer.decoder.class_embed.5.bias": 7,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 1792,
  "transformer.enc_out_class_embed.bias": 7,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/05 21:48:12.118]: Ignore keys: []
[05/05 21:48:12.206]: _IncompatibleKeys(missing_keys=['transformer.decoder.gesture_embed.weight', 'transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])
[05/05 21:49:27.237]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 21:49:27.240]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --person_only
[05/05 21:49:27.246]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 21:49:27.246]: world size: 1
[05/05 21:49:27.246]: rank: 0
[05/05 21:49:27.247]: local_rank: None
[05/05 21:49:27.247]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=True, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 21:49:28.603]: number of params:47883345
[05/05 21:49:28.613]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 256,
  "transformer.decoder.class_embed.0.bias": 1,
  "transformer.decoder.class_embed.1.weight": 256,
  "transformer.decoder.class_embed.1.bias": 1,
  "transformer.decoder.class_embed.2.weight": 256,
  "transformer.decoder.class_embed.2.bias": 1,
  "transformer.decoder.class_embed.3.weight": 256,
  "transformer.decoder.class_embed.3.bias": 1,
  "transformer.decoder.class_embed.4.weight": 256,
  "transformer.decoder.class_embed.4.bias": 1,
  "transformer.decoder.class_embed.5.weight": 256,
  "transformer.decoder.class_embed.5.bias": 1,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 256,
  "transformer.enc_out_class_embed.bias": 1,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/05 21:49:29.075]: Ignore keys: []
[05/05 21:49:29.163]: _IncompatibleKeys(missing_keys=['transformer.decoder.gesture_embed.weight', 'transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])
[05/05 21:51:17.813]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 21:51:17.816]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --person_only --device cpu
[05/05 21:51:17.821]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 21:51:17.822]: world size: 1
[05/05 21:51:17.822]: rank: 0
[05/05 21:51:17.822]: local_rank: None
[05/05 21:51:17.823]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cpu', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=True, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 21:51:19.205]: number of params:47883345
[05/05 21:51:19.215]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 256,
  "transformer.decoder.class_embed.0.bias": 1,
  "transformer.decoder.class_embed.1.weight": 256,
  "transformer.decoder.class_embed.1.bias": 1,
  "transformer.decoder.class_embed.2.weight": 256,
  "transformer.decoder.class_embed.2.bias": 1,
  "transformer.decoder.class_embed.3.weight": 256,
  "transformer.decoder.class_embed.3.bias": 1,
  "transformer.decoder.class_embed.4.weight": 256,
  "transformer.decoder.class_embed.4.bias": 1,
  "transformer.decoder.class_embed.5.weight": 256,
  "transformer.decoder.class_embed.5.bias": 1,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 256,
  "transformer.enc_out_class_embed.bias": 1,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/05 21:51:19.718]: Ignore keys: []
[05/05 21:51:19.821]: _IncompatibleKeys(missing_keys=['transformer.decoder.gesture_embed.weight', 'transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])
[05/05 21:52:17.182]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 21:52:17.185]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --person_only
[05/05 21:52:17.191]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 21:52:17.191]: world size: 1
[05/05 21:52:17.191]: rank: 0
[05/05 21:52:17.192]: local_rank: None
[05/05 21:52:17.192]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=True, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 21:52:18.498]: number of params:47883345
[05/05 21:52:18.509]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 256,
  "transformer.decoder.class_embed.0.bias": 1,
  "transformer.decoder.class_embed.1.weight": 256,
  "transformer.decoder.class_embed.1.bias": 1,
  "transformer.decoder.class_embed.2.weight": 256,
  "transformer.decoder.class_embed.2.bias": 1,
  "transformer.decoder.class_embed.3.weight": 256,
  "transformer.decoder.class_embed.3.bias": 1,
  "transformer.decoder.class_embed.4.weight": 256,
  "transformer.decoder.class_embed.4.bias": 1,
  "transformer.decoder.class_embed.5.weight": 256,
  "transformer.decoder.class_embed.5.bias": 1,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 256,
  "transformer.enc_out_class_embed.bias": 1,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/05 21:52:18.979]: Ignore keys: []
[05/05 21:52:19.067]: _IncompatibleKeys(missing_keys=['transformer.decoder.gesture_embed.weight', 'transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])
[05/05 22:05:47.037]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 22:05:47.040]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --person_only
[05/05 22:05:47.046]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 22:05:47.046]: world size: 1
[05/05 22:05:47.046]: rank: 0
[05/05 22:05:47.047]: local_rank: None
[05/05 22:05:47.048]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=True, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 22:05:48.435]: number of params:47883345
[05/05 22:05:48.445]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 256,
  "transformer.decoder.class_embed.0.bias": 1,
  "transformer.decoder.class_embed.1.weight": 256,
  "transformer.decoder.class_embed.1.bias": 1,
  "transformer.decoder.class_embed.2.weight": 256,
  "transformer.decoder.class_embed.2.bias": 1,
  "transformer.decoder.class_embed.3.weight": 256,
  "transformer.decoder.class_embed.3.bias": 1,
  "transformer.decoder.class_embed.4.weight": 256,
  "transformer.decoder.class_embed.4.bias": 1,
  "transformer.decoder.class_embed.5.weight": 256,
  "transformer.decoder.class_embed.5.bias": 1,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 256,
  "transformer.enc_out_class_embed.bias": 1,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/05 22:05:48.920]: Ignore keys: []
[05/05 22:05:49.010]: _IncompatibleKeys(missing_keys=['transformer.decoder.gesture_embed.weight', 'transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])
[05/05 22:07:16.665]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 22:07:16.669]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --person_only
[05/05 22:07:16.674]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 22:07:16.674]: world size: 1
[05/05 22:07:16.674]: rank: 0
[05/05 22:07:16.674]: local_rank: None
[05/05 22:07:16.675]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=True, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 22:07:18.087]: number of params:47883345
[05/05 22:07:18.097]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 256,
  "transformer.decoder.class_embed.0.bias": 1,
  "transformer.decoder.class_embed.1.weight": 256,
  "transformer.decoder.class_embed.1.bias": 1,
  "transformer.decoder.class_embed.2.weight": 256,
  "transformer.decoder.class_embed.2.bias": 1,
  "transformer.decoder.class_embed.3.weight": 256,
  "transformer.decoder.class_embed.3.bias": 1,
  "transformer.decoder.class_embed.4.weight": 256,
  "transformer.decoder.class_embed.4.bias": 1,
  "transformer.decoder.class_embed.5.weight": 256,
  "transformer.decoder.class_embed.5.bias": 1,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 256,
  "transformer.enc_out_class_embed.bias": 1,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/05 22:07:18.551]: Ignore keys: []
[05/05 22:07:18.639]: _IncompatibleKeys(missing_keys=['transformer.decoder.gesture_embed.weight', 'transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])
[05/05 22:17:46.661]: git:
  sha: d82cdb8e406c98a67d676c6e4a467d6e19de29fd, status: has uncommited changes, branch: master

[05/05 22:17:46.663]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=2 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --person_only
[05/05 22:17:46.669]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/05 22:17:46.670]: world size: 1
[05/05 22:17:46.670]: rank: 0
[05/05 22:17:46.671]: local_rank: None
[05/05 22:17:46.672]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=2, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/home/woody/iwi5/iwi5197h/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 2, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=True, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/05 22:17:48.044]: number of params:47883345
[05/05 22:17:48.054]: params:
{
  "transformer.level_embed": 1024,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.0.norm1.weight": 256,
  "transformer.encoder.layers.0.norm1.bias": 256,
  "transformer.encoder.layers.0.linear1.weight": 524288,
  "transformer.encoder.layers.0.linear1.bias": 2048,
  "transformer.encoder.layers.0.linear2.weight": 524288,
  "transformer.encoder.layers.0.linear2.bias": 256,
  "transformer.encoder.layers.0.norm2.weight": 256,
  "transformer.encoder.layers.0.norm2.bias": 256,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.1.norm1.weight": 256,
  "transformer.encoder.layers.1.norm1.bias": 256,
  "transformer.encoder.layers.1.linear1.weight": 524288,
  "transformer.encoder.layers.1.linear1.bias": 2048,
  "transformer.encoder.layers.1.linear2.weight": 524288,
  "transformer.encoder.layers.1.linear2.bias": 256,
  "transformer.encoder.layers.1.norm2.weight": 256,
  "transformer.encoder.layers.1.norm2.bias": 256,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.2.norm1.weight": 256,
  "transformer.encoder.layers.2.norm1.bias": 256,
  "transformer.encoder.layers.2.linear1.weight": 524288,
  "transformer.encoder.layers.2.linear1.bias": 2048,
  "transformer.encoder.layers.2.linear2.weight": 524288,
  "transformer.encoder.layers.2.linear2.bias": 256,
  "transformer.encoder.layers.2.norm2.weight": 256,
  "transformer.encoder.layers.2.norm2.bias": 256,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.3.norm1.weight": 256,
  "transformer.encoder.layers.3.norm1.bias": 256,
  "transformer.encoder.layers.3.linear1.weight": 524288,
  "transformer.encoder.layers.3.linear1.bias": 2048,
  "transformer.encoder.layers.3.linear2.weight": 524288,
  "transformer.encoder.layers.3.linear2.bias": 256,
  "transformer.encoder.layers.3.norm2.weight": 256,
  "transformer.encoder.layers.3.norm2.bias": 256,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.4.norm1.weight": 256,
  "transformer.encoder.layers.4.norm1.bias": 256,
  "transformer.encoder.layers.4.linear1.weight": 524288,
  "transformer.encoder.layers.4.linear1.bias": 2048,
  "transformer.encoder.layers.4.linear2.weight": 524288,
  "transformer.encoder.layers.4.linear2.bias": 256,
  "transformer.encoder.layers.4.norm2.weight": 256,
  "transformer.encoder.layers.4.norm2.bias": 256,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "transformer.encoder.layers.5.norm1.weight": 256,
  "transformer.encoder.layers.5.norm1.bias": 256,
  "transformer.encoder.layers.5.linear1.weight": 524288,
  "transformer.encoder.layers.5.linear1.bias": 2048,
  "transformer.encoder.layers.5.linear2.weight": 524288,
  "transformer.encoder.layers.5.linear2.bias": 256,
  "transformer.encoder.layers.5.norm2.weight": 256,
  "transformer.encoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.0.norm1.weight": 256,
  "transformer.decoder.layers.0.norm1.bias": 256,
  "transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.0.norm2.weight": 256,
  "transformer.decoder.layers.0.norm2.bias": 256,
  "transformer.decoder.layers.0.linear1.weight": 524288,
  "transformer.decoder.layers.0.linear1.bias": 2048,
  "transformer.decoder.layers.0.linear2.weight": 524288,
  "transformer.decoder.layers.0.linear2.bias": 256,
  "transformer.decoder.layers.0.norm3.weight": 256,
  "transformer.decoder.layers.0.norm3.bias": 256,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.1.norm1.weight": 256,
  "transformer.decoder.layers.1.norm1.bias": 256,
  "transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.1.norm2.weight": 256,
  "transformer.decoder.layers.1.norm2.bias": 256,
  "transformer.decoder.layers.1.linear1.weight": 524288,
  "transformer.decoder.layers.1.linear1.bias": 2048,
  "transformer.decoder.layers.1.linear2.weight": 524288,
  "transformer.decoder.layers.1.linear2.bias": 256,
  "transformer.decoder.layers.1.norm3.weight": 256,
  "transformer.decoder.layers.1.norm3.bias": 256,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.2.norm1.weight": 256,
  "transformer.decoder.layers.2.norm1.bias": 256,
  "transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.2.norm2.weight": 256,
  "transformer.decoder.layers.2.norm2.bias": 256,
  "transformer.decoder.layers.2.linear1.weight": 524288,
  "transformer.decoder.layers.2.linear1.bias": 2048,
  "transformer.decoder.layers.2.linear2.weight": 524288,
  "transformer.decoder.layers.2.linear2.bias": 256,
  "transformer.decoder.layers.2.norm3.weight": 256,
  "transformer.decoder.layers.2.norm3.bias": 256,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.3.norm1.weight": 256,
  "transformer.decoder.layers.3.norm1.bias": 256,
  "transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.3.norm2.weight": 256,
  "transformer.decoder.layers.3.norm2.bias": 256,
  "transformer.decoder.layers.3.linear1.weight": 524288,
  "transformer.decoder.layers.3.linear1.bias": 2048,
  "transformer.decoder.layers.3.linear2.weight": 524288,
  "transformer.decoder.layers.3.linear2.bias": 256,
  "transformer.decoder.layers.3.norm3.weight": 256,
  "transformer.decoder.layers.3.norm3.bias": 256,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.4.norm1.weight": 256,
  "transformer.decoder.layers.4.norm1.bias": 256,
  "transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.4.norm2.weight": 256,
  "transformer.decoder.layers.4.norm2.bias": 256,
  "transformer.decoder.layers.4.linear1.weight": 524288,
  "transformer.decoder.layers.4.linear1.bias": 2048,
  "transformer.decoder.layers.4.linear2.weight": 524288,
  "transformer.decoder.layers.4.linear2.bias": 256,
  "transformer.decoder.layers.4.norm3.weight": 256,
  "transformer.decoder.layers.4.norm3.bias": 256,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "transformer.decoder.layers.5.norm1.weight": 256,
  "transformer.decoder.layers.5.norm1.bias": 256,
  "transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "transformer.decoder.layers.5.norm2.weight": 256,
  "transformer.decoder.layers.5.norm2.bias": 256,
  "transformer.decoder.layers.5.linear1.weight": 524288,
  "transformer.decoder.layers.5.linear1.bias": 2048,
  "transformer.decoder.layers.5.linear2.weight": 524288,
  "transformer.decoder.layers.5.linear2.bias": 256,
  "transformer.decoder.layers.5.norm3.weight": 256,
  "transformer.decoder.layers.5.norm3.bias": 256,
  "transformer.decoder.norm.weight": 256,
  "transformer.decoder.norm.bias": 256,
  "transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "transformer.decoder.ref_point_head.layers.0.bias": 256,
  "transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "transformer.decoder.ref_point_head.layers.1.bias": 256,
  "transformer.decoder.hw.weight": 34,
  "transformer.decoder.keypoint_embed.weight": 4352,
  "transformer.decoder.gesture_embed.weight": 25600,
  "transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "transformer.decoder.class_embed.0.weight": 256,
  "transformer.decoder.class_embed.0.bias": 1,
  "transformer.decoder.class_embed.1.weight": 256,
  "transformer.decoder.class_embed.1.bias": 1,
  "transformer.decoder.class_embed.2.weight": 256,
  "transformer.decoder.class_embed.2.bias": 1,
  "transformer.decoder.class_embed.3.weight": 256,
  "transformer.decoder.class_embed.3.bias": 1,
  "transformer.decoder.class_embed.4.weight": 256,
  "transformer.decoder.class_embed.4.bias": 1,
  "transformer.decoder.class_embed.5.weight": 256,
  "transformer.decoder.class_embed.5.bias": 1,
  "transformer.enc_output.weight": 65536,
  "transformer.enc_output.bias": 256,
  "transformer.enc_output_norm.weight": 256,
  "transformer.enc_output_norm.bias": 256,
  "transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "transformer.enc_out_class_embed.weight": 256,
  "transformer.enc_out_class_embed.bias": 1,
  "label_enc.weight": 25856,
  "input_proj.0.0.weight": 131072,
  "input_proj.0.0.bias": 256,
  "input_proj.0.1.weight": 256,
  "input_proj.0.1.bias": 256,
  "input_proj.1.0.weight": 262144,
  "input_proj.1.0.bias": 256,
  "input_proj.1.1.weight": 256,
  "input_proj.1.1.bias": 256,
  "input_proj.2.0.weight": 524288,
  "input_proj.2.0.bias": 256,
  "input_proj.2.1.weight": 256,
  "input_proj.2.1.bias": 256,
  "input_proj.3.0.weight": 4718592,
  "input_proj.3.0.bias": 256,
  "input_proj.3.1.weight": 256,
  "input_proj.3.1.bias": 256,
  "backbone.0.body.layer2.0.conv1.weight": 32768,
  "backbone.0.body.layer2.0.conv2.weight": 147456,
  "backbone.0.body.layer2.0.conv3.weight": 65536,
  "backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "backbone.0.body.layer2.1.conv1.weight": 65536,
  "backbone.0.body.layer2.1.conv2.weight": 147456,
  "backbone.0.body.layer2.1.conv3.weight": 65536,
  "backbone.0.body.layer2.2.conv1.weight": 65536,
  "backbone.0.body.layer2.2.conv2.weight": 147456,
  "backbone.0.body.layer2.2.conv3.weight": 65536,
  "backbone.0.body.layer2.3.conv1.weight": 65536,
  "backbone.0.body.layer2.3.conv2.weight": 147456,
  "backbone.0.body.layer2.3.conv3.weight": 65536,
  "backbone.0.body.layer3.0.conv1.weight": 131072,
  "backbone.0.body.layer3.0.conv2.weight": 589824,
  "backbone.0.body.layer3.0.conv3.weight": 262144,
  "backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "backbone.0.body.layer3.1.conv1.weight": 262144,
  "backbone.0.body.layer3.1.conv2.weight": 589824,
  "backbone.0.body.layer3.1.conv3.weight": 262144,
  "backbone.0.body.layer3.2.conv1.weight": 262144,
  "backbone.0.body.layer3.2.conv2.weight": 589824,
  "backbone.0.body.layer3.2.conv3.weight": 262144,
  "backbone.0.body.layer3.3.conv1.weight": 262144,
  "backbone.0.body.layer3.3.conv2.weight": 589824,
  "backbone.0.body.layer3.3.conv3.weight": 262144,
  "backbone.0.body.layer3.4.conv1.weight": 262144,
  "backbone.0.body.layer3.4.conv2.weight": 589824,
  "backbone.0.body.layer3.4.conv3.weight": 262144,
  "backbone.0.body.layer3.5.conv1.weight": 262144,
  "backbone.0.body.layer3.5.conv2.weight": 589824,
  "backbone.0.body.layer3.5.conv3.weight": 262144,
  "backbone.0.body.layer4.0.conv1.weight": 524288,
  "backbone.0.body.layer4.0.conv2.weight": 2359296,
  "backbone.0.body.layer4.0.conv3.weight": 1048576,
  "backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "backbone.0.body.layer4.1.conv1.weight": 1048576,
  "backbone.0.body.layer4.1.conv2.weight": 2359296,
  "backbone.0.body.layer4.1.conv3.weight": 1048576,
  "backbone.0.body.layer4.2.conv1.weight": 1048576,
  "backbone.0.body.layer4.2.conv2.weight": 2359296,
  "backbone.0.body.layer4.2.conv3.weight": 1048576
}
[05/21 15:50:31.445]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 15:50:31.447]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=1 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --dec_layers 7
[05/21 15:50:31.455]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 15:50:31.456]: world size: 1
[05/21 15:50:31.457]: rank: 0
[05/21 15:50:31.457]: local_rank: None
[05/21 15:50:31.459]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=1, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path=None, config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=7, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 1, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 15:53:03.642]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 15:53:03.644]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=1 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --dec_layers 7
[05/21 15:53:03.653]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 15:53:03.654]: world size: 1
[05/21 15:53:03.654]: rank: 0
[05/21 15:53:03.654]: local_rank: None
[05/21 15:53:03.656]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=1, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path=None, config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=7, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 1, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 15:53:55.732]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 15:53:55.734]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=1 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --pretrain_model_path ./models/edpose_r50_coco.pth --finetune_ignore class_embed. --dec_layers 8
[05/21 15:53:55.743]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 15:53:55.743]: world size: 1
[05/21 15:53:55.744]: rank: 0
[05/21 15:53:55.744]: local_rank: None
[05/21 15:53:55.745]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=1, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=False, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path=None, config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=8, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=None, edpose_model_path=None, ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=['class_embed.'], fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 1, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path='./models/edpose_r50_coco.pth', query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=False, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 17:45:21.618]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 17:45:21.619]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2
[05/21 17:45:21.628]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 17:45:21.628]: world size: 1
[05/21 17:45:21.629]: rank: 0
[05/21 17:45:21.629]: local_rank: None
[05/21 17:45:21.630]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path=None, config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 17:45:31.347]: number of params:51450506
[05/21 17:45:31.357]: params:
{
  "edpose_model.transformer.level_embed": 1024,
  "edpose_model.transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.0.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.0.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.0.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.0.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.0.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.0.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.0.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.0.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.1.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.1.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.1.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.1.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.1.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.1.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.1.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.1.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.2.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.2.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.2.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.2.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.2.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.2.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.2.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.2.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.3.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.3.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.3.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.3.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.3.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.3.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.3.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.3.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.4.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.4.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.4.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.4.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.4.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.4.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.4.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.4.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.5.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.5.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.5.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.5.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.5.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.5.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.5.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.5.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.0.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.0.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.0.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.1.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.1.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.1.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.1.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.2.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.2.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.2.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.2.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.3.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.3.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.3.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.3.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.4.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.4.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.4.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.4.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.5.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.5.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.5.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.5.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm3.bias": 256,
  "edpose_model.transformer.decoder.norm.weight": 256,
  "edpose_model.transformer.decoder.norm.bias": 256,
  "edpose_model.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "edpose_model.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "edpose_model.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "edpose_model.transformer.decoder.hw.weight": 34,
  "edpose_model.transformer.decoder.keypoint_embed.weight": 4352,
  "edpose_model.transformer.decoder.gesture_embed.weight": 25600,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "edpose_model.transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "edpose_model.transformer.decoder.class_embed.0.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.0.bias": 7,
  "edpose_model.transformer.decoder.class_embed.1.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.1.bias": 7,
  "edpose_model.transformer.decoder.class_embed.2.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.2.bias": 7,
  "edpose_model.transformer.decoder.class_embed.3.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.3.bias": 7,
  "edpose_model.transformer.decoder.class_embed.4.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.4.bias": 7,
  "edpose_model.transformer.decoder.class_embed.5.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.5.bias": 7,
  "edpose_model.transformer.enc_output.weight": 65536,
  "edpose_model.transformer.enc_output.bias": 256,
  "edpose_model.transformer.enc_output_norm.weight": 256,
  "edpose_model.transformer.enc_output_norm.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "edpose_model.transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "edpose_model.transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "edpose_model.transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "edpose_model.transformer.enc_out_class_embed.weight": 1792,
  "edpose_model.transformer.enc_out_class_embed.bias": 7,
  "edpose_model.label_enc.weight": 25856,
  "edpose_model.input_proj.0.0.weight": 131072,
  "edpose_model.input_proj.0.0.bias": 256,
  "edpose_model.input_proj.0.1.weight": 256,
  "edpose_model.input_proj.0.1.bias": 256,
  "edpose_model.input_proj.1.0.weight": 262144,
  "edpose_model.input_proj.1.0.bias": 256,
  "edpose_model.input_proj.1.1.weight": 256,
  "edpose_model.input_proj.1.1.bias": 256,
  "edpose_model.input_proj.2.0.weight": 524288,
  "edpose_model.input_proj.2.0.bias": 256,
  "edpose_model.input_proj.2.1.weight": 256,
  "edpose_model.input_proj.2.1.bias": 256,
  "edpose_model.input_proj.3.0.weight": 4718592,
  "edpose_model.input_proj.3.0.bias": 256,
  "edpose_model.input_proj.3.1.weight": 256,
  "edpose_model.input_proj.3.1.bias": 256,
  "edpose_model.backbone.0.body.layer2.0.conv1.weight": 32768,
  "edpose_model.backbone.0.body.layer2.0.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.0.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "edpose_model.backbone.0.body.layer2.1.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.1.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.1.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.2.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.2.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.2.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.3.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.3.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.3.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer3.0.conv1.weight": 131072,
  "edpose_model.backbone.0.body.layer3.0.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.0.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "edpose_model.backbone.0.body.layer3.1.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.1.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.1.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.2.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.2.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.2.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.3.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.3.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.3.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.4.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.4.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.4.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.5.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.5.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.5.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer4.0.conv1.weight": 524288,
  "edpose_model.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "edpose_model.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.2.conv3.weight": 1048576,
  "decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "decoder.layers.0.cross_attn.value_proj.bias": 256,
  "decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "decoder.layers.0.cross_attn.output_proj.bias": 256,
  "decoder.layers.0.norm1.weight": 256,
  "decoder.layers.0.norm1.bias": 256,
  "decoder.layers.0.self_attn.in_proj_weight": 196608,
  "decoder.layers.0.self_attn.in_proj_bias": 768,
  "decoder.layers.0.self_attn.out_proj.weight": 65536,
  "decoder.layers.0.self_attn.out_proj.bias": 256,
  "decoder.layers.0.norm2.weight": 256,
  "decoder.layers.0.norm2.bias": 256,
  "decoder.layers.0.linear1.weight": 524288,
  "decoder.layers.0.linear1.bias": 2048,
  "decoder.layers.0.linear2.weight": 524288,
  "decoder.layers.0.linear2.bias": 256,
  "decoder.layers.0.norm3.weight": 256,
  "decoder.layers.0.norm3.bias": 256,
  "decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "decoder.layers.1.cross_attn.value_proj.bias": 256,
  "decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "decoder.layers.1.cross_attn.output_proj.bias": 256,
  "decoder.layers.1.norm1.weight": 256,
  "decoder.layers.1.norm1.bias": 256,
  "decoder.layers.1.self_attn.in_proj_weight": 196608,
  "decoder.layers.1.self_attn.in_proj_bias": 768,
  "decoder.layers.1.self_attn.out_proj.weight": 65536,
  "decoder.layers.1.self_attn.out_proj.bias": 256,
  "decoder.layers.1.norm2.weight": 256,
  "decoder.layers.1.norm2.bias": 256,
  "decoder.layers.1.linear1.weight": 524288,
  "decoder.layers.1.linear1.bias": 2048,
  "decoder.layers.1.linear2.weight": 524288,
  "decoder.layers.1.linear2.bias": 256,
  "decoder.layers.1.norm3.weight": 256,
  "decoder.layers.1.norm3.bias": 256,
  "decoder.norm.weight": 256,
  "decoder.norm.bias": 256,
  "decoder.ref_point_head.layers.0.weight": 131072,
  "decoder.ref_point_head.layers.0.bias": 256,
  "decoder.ref_point_head.layers.1.weight": 65536,
  "decoder.ref_point_head.layers.1.bias": 256,
  "decoder.bbox_embed.0.layers.0.weight": 65536,
  "decoder.bbox_embed.0.layers.0.bias": 256,
  "decoder.bbox_embed.0.layers.1.weight": 65536,
  "decoder.bbox_embed.0.layers.1.bias": 256,
  "decoder.bbox_embed.0.layers.2.weight": 1024,
  "decoder.bbox_embed.0.layers.2.bias": 4,
  "decoder.bbox_embed.1.layers.0.weight": 65536,
  "decoder.bbox_embed.1.layers.0.bias": 256,
  "decoder.bbox_embed.1.layers.1.weight": 65536,
  "decoder.bbox_embed.1.layers.1.bias": 256,
  "decoder.bbox_embed.1.layers.2.weight": 1024,
  "decoder.bbox_embed.1.layers.2.bias": 4,
  "class_embed.weight": 1792,
  "class_embed.bias": 7
}
[05/21 17:47:29.047]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 17:47:29.049]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2
[05/21 17:47:29.057]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 17:47:29.057]: world size: 1
[05/21 17:47:29.058]: rank: 0
[05/21 17:47:29.059]: local_rank: None
[05/21 17:47:29.060]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 17:47:30.396]: number of params:51450506
[05/21 17:47:30.405]: params:
{
  "edpose_model.transformer.level_embed": 1024,
  "edpose_model.transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.0.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.0.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.0.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.0.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.0.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.0.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.0.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.0.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.1.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.1.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.1.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.1.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.1.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.1.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.1.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.1.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.2.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.2.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.2.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.2.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.2.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.2.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.2.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.2.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.3.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.3.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.3.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.3.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.3.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.3.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.3.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.3.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.4.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.4.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.4.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.4.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.4.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.4.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.4.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.4.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.5.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.5.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.5.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.5.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.5.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.5.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.5.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.5.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.0.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.0.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.0.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.1.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.1.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.1.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.1.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.2.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.2.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.2.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.2.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.3.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.3.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.3.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.3.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.4.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.4.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.4.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.4.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.5.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.5.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.5.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.5.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm3.bias": 256,
  "edpose_model.transformer.decoder.norm.weight": 256,
  "edpose_model.transformer.decoder.norm.bias": 256,
  "edpose_model.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "edpose_model.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "edpose_model.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "edpose_model.transformer.decoder.hw.weight": 34,
  "edpose_model.transformer.decoder.keypoint_embed.weight": 4352,
  "edpose_model.transformer.decoder.gesture_embed.weight": 25600,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "edpose_model.transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "edpose_model.transformer.decoder.class_embed.0.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.0.bias": 7,
  "edpose_model.transformer.decoder.class_embed.1.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.1.bias": 7,
  "edpose_model.transformer.decoder.class_embed.2.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.2.bias": 7,
  "edpose_model.transformer.decoder.class_embed.3.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.3.bias": 7,
  "edpose_model.transformer.decoder.class_embed.4.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.4.bias": 7,
  "edpose_model.transformer.decoder.class_embed.5.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.5.bias": 7,
  "edpose_model.transformer.enc_output.weight": 65536,
  "edpose_model.transformer.enc_output.bias": 256,
  "edpose_model.transformer.enc_output_norm.weight": 256,
  "edpose_model.transformer.enc_output_norm.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "edpose_model.transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "edpose_model.transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "edpose_model.transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "edpose_model.transformer.enc_out_class_embed.weight": 1792,
  "edpose_model.transformer.enc_out_class_embed.bias": 7,
  "edpose_model.label_enc.weight": 25856,
  "edpose_model.input_proj.0.0.weight": 131072,
  "edpose_model.input_proj.0.0.bias": 256,
  "edpose_model.input_proj.0.1.weight": 256,
  "edpose_model.input_proj.0.1.bias": 256,
  "edpose_model.input_proj.1.0.weight": 262144,
  "edpose_model.input_proj.1.0.bias": 256,
  "edpose_model.input_proj.1.1.weight": 256,
  "edpose_model.input_proj.1.1.bias": 256,
  "edpose_model.input_proj.2.0.weight": 524288,
  "edpose_model.input_proj.2.0.bias": 256,
  "edpose_model.input_proj.2.1.weight": 256,
  "edpose_model.input_proj.2.1.bias": 256,
  "edpose_model.input_proj.3.0.weight": 4718592,
  "edpose_model.input_proj.3.0.bias": 256,
  "edpose_model.input_proj.3.1.weight": 256,
  "edpose_model.input_proj.3.1.bias": 256,
  "edpose_model.backbone.0.body.layer2.0.conv1.weight": 32768,
  "edpose_model.backbone.0.body.layer2.0.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.0.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "edpose_model.backbone.0.body.layer2.1.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.1.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.1.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.2.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.2.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.2.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.3.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.3.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.3.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer3.0.conv1.weight": 131072,
  "edpose_model.backbone.0.body.layer3.0.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.0.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "edpose_model.backbone.0.body.layer3.1.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.1.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.1.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.2.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.2.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.2.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.3.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.3.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.3.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.4.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.4.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.4.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.5.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.5.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.5.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer4.0.conv1.weight": 524288,
  "edpose_model.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "edpose_model.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.2.conv3.weight": 1048576,
  "decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "decoder.layers.0.cross_attn.value_proj.bias": 256,
  "decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "decoder.layers.0.cross_attn.output_proj.bias": 256,
  "decoder.layers.0.norm1.weight": 256,
  "decoder.layers.0.norm1.bias": 256,
  "decoder.layers.0.self_attn.in_proj_weight": 196608,
  "decoder.layers.0.self_attn.in_proj_bias": 768,
  "decoder.layers.0.self_attn.out_proj.weight": 65536,
  "decoder.layers.0.self_attn.out_proj.bias": 256,
  "decoder.layers.0.norm2.weight": 256,
  "decoder.layers.0.norm2.bias": 256,
  "decoder.layers.0.linear1.weight": 524288,
  "decoder.layers.0.linear1.bias": 2048,
  "decoder.layers.0.linear2.weight": 524288,
  "decoder.layers.0.linear2.bias": 256,
  "decoder.layers.0.norm3.weight": 256,
  "decoder.layers.0.norm3.bias": 256,
  "decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "decoder.layers.1.cross_attn.value_proj.bias": 256,
  "decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "decoder.layers.1.cross_attn.output_proj.bias": 256,
  "decoder.layers.1.norm1.weight": 256,
  "decoder.layers.1.norm1.bias": 256,
  "decoder.layers.1.self_attn.in_proj_weight": 196608,
  "decoder.layers.1.self_attn.in_proj_bias": 768,
  "decoder.layers.1.self_attn.out_proj.weight": 65536,
  "decoder.layers.1.self_attn.out_proj.bias": 256,
  "decoder.layers.1.norm2.weight": 256,
  "decoder.layers.1.norm2.bias": 256,
  "decoder.layers.1.linear1.weight": 524288,
  "decoder.layers.1.linear1.bias": 2048,
  "decoder.layers.1.linear2.weight": 524288,
  "decoder.layers.1.linear2.bias": 256,
  "decoder.layers.1.norm3.weight": 256,
  "decoder.layers.1.norm3.bias": 256,
  "decoder.norm.weight": 256,
  "decoder.norm.bias": 256,
  "decoder.ref_point_head.layers.0.weight": 131072,
  "decoder.ref_point_head.layers.0.bias": 256,
  "decoder.ref_point_head.layers.1.weight": 65536,
  "decoder.ref_point_head.layers.1.bias": 256,
  "decoder.bbox_embed.0.layers.0.weight": 65536,
  "decoder.bbox_embed.0.layers.0.bias": 256,
  "decoder.bbox_embed.0.layers.1.weight": 65536,
  "decoder.bbox_embed.0.layers.1.bias": 256,
  "decoder.bbox_embed.0.layers.2.weight": 1024,
  "decoder.bbox_embed.0.layers.2.bias": 4,
  "decoder.bbox_embed.1.layers.0.weight": 65536,
  "decoder.bbox_embed.1.layers.0.bias": 256,
  "decoder.bbox_embed.1.layers.1.weight": 65536,
  "decoder.bbox_embed.1.layers.1.bias": 256,
  "decoder.bbox_embed.1.layers.2.weight": 1024,
  "decoder.bbox_embed.1.layers.2.bias": 4,
  "class_embed.weight": 1792,
  "class_embed.bias": 7
}
[05/21 18:24:46.837]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 18:24:46.839]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2 --classifier_type full
[05/21 18:24:46.847]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 18:24:46.847]: world size: 1
[05/21 18:24:46.848]: rank: 0
[05/21 18:24:46.849]: local_rank: None
[05/21 18:24:46.850]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_type='full', classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 18:27:42.062]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 18:27:42.064]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2 --classifier_type full
[05/21 18:27:42.073]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 18:27:42.073]: world size: 1
[05/21 18:27:42.074]: rank: 0
[05/21 18:27:42.074]: local_rank: None
[05/21 18:27:42.076]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_type='full', classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 18:29:28.208]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 18:29:28.210]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2 --classifier_type full
[05/21 18:29:28.218]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 18:29:28.218]: world size: 1
[05/21 18:29:28.219]: rank: 0
[05/21 18:29:28.219]: local_rank: None
[05/21 18:29:28.220]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_type='full', classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 18:31:03.403]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 18:31:03.405]: Command: main.py --config_file config/edpose.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2 --classifier_type full
[05/21 18:31:03.413]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 18:31:03.414]: world size: 1
[05/21 18:31:03.414]: rank: 0
[05/21 18:31:03.415]: local_rank: None
[05/21 18:31:03.416]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_type='full', classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/edpose.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='edpose', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 18:34:51.076]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 18:34:51.079]: Command: main.py --config_file config/classifier_full.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2 --classifier_type full
[05/21 18:34:51.086]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 18:34:51.087]: world size: 1
[05/21 18:34:51.087]: rank: 0
[05/21 18:34:51.087]: local_rank: None
[05/21 18:34:51.088]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_type='full', classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/classifier_full.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='classifier_full', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 18:36:34.022]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 18:36:34.024]: Command: main.py --config_file config/classifier_full.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2 --classifier_type full
[05/21 18:36:34.032]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 18:36:34.032]: world size: 1
[05/21 18:36:34.033]: rank: 0
[05/21 18:36:34.033]: local_rank: None
[05/21 18:36:34.034]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_type='full', classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/classifier_full.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='classifier_full', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 19:08:44.420]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 19:08:44.422]: Command: main.py --config_file config/classifier.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2 --classifier_type full
[05/21 19:08:44.428]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 19:08:44.428]: world size: 1
[05/21 19:08:44.428]: rank: 0
[05/21 19:08:44.429]: local_rank: None
[05/21 19:08:44.430]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_type='full', classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/classifier.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='classifier', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 19:09:28.214]: git:
  sha: a7913fc6c16e21fc5d73b3027a7562d16dc5402a, status: has uncommited changes, branch: master

[05/21 19:09:28.217]: Command: main.py --config_file config/classifier.cfg.py --output_dir logs/train/humanart_debug/ --options batch_size=4 epochs=20 lr_drop=6 num_body_points=17 backbone=resnet50 --dataset_file coco --edpose_model_path ./models/edpose_r50_coco.pth --edpose_finetune_ignore class_embed. --seperate_classifier --classifier_use_deformable --classifier_decoder_layers 2 --classifier_type full
[05/21 19:09:28.227]: Full config saved to logs/train/humanart_debug/config_args_all.json
[05/21 19:09:28.227]: world size: 1
[05/21 19:09:28.228]: rank: 0
[05/21 19:09:28.228]: local_rank: None
[05/21 19:09:28.229]: args: Namespace(amp=False, aux_loss=True, backbone='resnet50', backbone_freeze_keywords=None, batch_norm_type='FrozenBatchNorm2d', batch_size=4, bbox_loss_coef=5.0, classifier_decoder_layers=2, classifier_decoder_return_intermediate=False, classifier_type='full', classifier_use_deformable=True, clip_max_norm=0.1, cls_loss_coef=2.0, cls_no_bias=False, coco_path='/net/cluster/azhar/datasets/gestuers_keypoints_cmac/smell-gesture-recognition/coco_directory_gestures', config_file='config/classifier.cfg.py', data_aug_max_size=1333, data_aug_scale_overlap=None, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_scales2_crop=[384, 600], data_aug_scales2_resize=[400, 500, 600], dataset_file='coco', ddetr_lr_param=False, debug=False, dec_layer_number=None, dec_layers=6, dec_n_points=4, dec_pred_bbox_embed_share=False, dec_pred_class_embed_share=False, dec_pred_pose_embed_share=False, decoder_box_layers=2, decoder_module_seq=['sa', 'ca', 'ffn'], decoder_sa_type='sa', device='cuda', dilation=False, dim_feedforward=2048, dist_url='env://', distributed=False, dln_hw_noise=0.2, dln_xy_noise=0.2, dn_attn_mask_type_list=['match2dn', 'dn2dn', 'group2group'], dn_batch_gt_fuse=False, dn_bbox_coef=0.5, dn_box_noise_scale=0.4, dn_label_coef=0.3, dn_label_noise_ratio=0.5, dn_labelbook_size=100, dn_number=100, dropout=0.0, edpose_finetune_ignore=['class_embed.'], edpose_model_path='./models/edpose_r50_coco.pth', ema_decay=0.9997, ema_epoch=0, embed_init_tgt=False, enc_layers=6, enc_loss_coef=1.0, enc_n_points=4, epochs=20, eval=False, find_unused_params=False, finetune_edpose=False, finetune_ignore=None, fix_refpoints_hw=-1, fix_size=False, focal_alpha=0.25, frozen_weights=None, giou_loss_coef=2.0, hidden_dim=256, indices_idx_list=[1, 2, 3, 4, 5, 6, 7], interm_loss_coef=1.0, keypoints_loss_coef=10.0, local_rank=None, lr=0.0001, lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_drop=6, lr_drop_list=[33, 45], lr_linear_proj_mult=0.1, lr_linear_proj_names=['reference_points', 'sampling_offsets'], masks=False, match_unstable_error=False, matcher_type='HungarianMatcher', modelname='classifier', multi_step_lr=False, nheads=8, nms_iou_threshold=-1, no_dn=False, no_interm_box_loss=False, no_mmpose_keypoint_evaluator=True, note='', num_body_points=17, num_box_decoder_layers=2, num_classes=7, num_feature_levels=4, num_group=100, num_patterns=0, num_queries=900, num_select=50, num_workers=0, oks_loss_coef=4.0, onecyclelr=False, options={'batch_size': 4, 'epochs': 20, 'lr_drop': 6, 'num_body_points': 17, 'backbone': 'resnet50'}, output_dir='logs/train/humanart_debug/', param_dict_type='default', pe_temperatureH=20, pe_temperatureW=20, person_only=False, position_embedding='sine', pre_norm=False, pretrain_model_path=None, query_dim=4, random_refpoints_xy=False, rank=0, remove_difficult=False, resume='', return_interm_indices=[1, 2, 3], rm_detach=None, rm_self_attn_layers=None, save_checkpoint_interval=100, save_log=False, save_results=False, seed=42, seperate_classifier=True, seperate_token_for_class=False, set_cost_bbox=5.0, set_cost_class=2.0, set_cost_giou=2.0, set_cost_keypoints=10.0, set_cost_kpvis=0.0, set_cost_oks=4.0, start_epoch=0, strong_aug=False, test=False, transformer_activation='relu', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, two_stage_learn_wh=False, two_stage_type='standard', use_checkpoint=False, use_dn=True, use_ema=True, weight_decay=0.0001, world_size=1)

[05/21 19:09:29.583]: number of params:52630702
[05/21 19:09:29.593]: params:
{
  "edpose_model.transformer.level_embed": 1024,
  "edpose_model.transformer.encoder.layers.0.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.0.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.0.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.0.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.0.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.0.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.0.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.0.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.0.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.0.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.0.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.0.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.0.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.0.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.1.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.1.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.1.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.1.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.1.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.1.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.1.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.1.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.1.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.1.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.1.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.1.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.2.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.2.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.2.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.2.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.2.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.2.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.2.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.2.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.2.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.2.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.2.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.2.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.3.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.3.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.3.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.3.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.3.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.3.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.3.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.3.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.3.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.3.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.3.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.3.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.4.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.4.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.4.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.4.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.4.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.4.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.4.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.4.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.4.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.4.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.4.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.4.norm2.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.encoder.layers.5.self_attn.attention_weights.bias": 128,
  "edpose_model.transformer.encoder.layers.5.self_attn.value_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.value_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.5.self_attn.output_proj.weight": 65536,
  "edpose_model.transformer.encoder.layers.5.self_attn.output_proj.bias": 256,
  "edpose_model.transformer.encoder.layers.5.norm1.weight": 256,
  "edpose_model.transformer.encoder.layers.5.norm1.bias": 256,
  "edpose_model.transformer.encoder.layers.5.linear1.weight": 524288,
  "edpose_model.transformer.encoder.layers.5.linear1.bias": 2048,
  "edpose_model.transformer.encoder.layers.5.linear2.weight": 524288,
  "edpose_model.transformer.encoder.layers.5.linear2.bias": 256,
  "edpose_model.transformer.encoder.layers.5.norm2.weight": 256,
  "edpose_model.transformer.encoder.layers.5.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.0.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.0.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.0.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.0.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.0.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.0.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.0.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.0.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.0.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.1.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.1.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.1.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.1.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.1.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.1.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.1.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.1.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.1.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.1.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.2.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.2.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.2.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.2.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.2.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.2.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.2.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.2.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.2.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.2.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.2.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.2.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.3.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.3.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.3.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.3.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.3.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.3.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.3.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.3.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.3.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.3.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.3.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.3.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.4.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.4.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.4.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.4.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.4.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.4.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.4.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.4.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.4.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.4.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.4.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.4.norm3.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.attention_weights.weight": 32768,
  "edpose_model.transformer.decoder.layers.5.cross_attn.attention_weights.bias": 128,
  "edpose_model.transformer.decoder.layers.5.cross_attn.value_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.value_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.cross_attn.output_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.cross_attn.output_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm1.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm1.bias": 256,
  "edpose_model.transformer.decoder.layers.5.self_attn.in_proj_weight": 196608,
  "edpose_model.transformer.decoder.layers.5.self_attn.in_proj_bias": 768,
  "edpose_model.transformer.decoder.layers.5.self_attn.out_proj.weight": 65536,
  "edpose_model.transformer.decoder.layers.5.self_attn.out_proj.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm2.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm2.bias": 256,
  "edpose_model.transformer.decoder.layers.5.linear1.weight": 524288,
  "edpose_model.transformer.decoder.layers.5.linear1.bias": 2048,
  "edpose_model.transformer.decoder.layers.5.linear2.weight": 524288,
  "edpose_model.transformer.decoder.layers.5.linear2.bias": 256,
  "edpose_model.transformer.decoder.layers.5.norm3.weight": 256,
  "edpose_model.transformer.decoder.layers.5.norm3.bias": 256,
  "edpose_model.transformer.decoder.norm.weight": 256,
  "edpose_model.transformer.decoder.norm.bias": 256,
  "edpose_model.transformer.decoder.ref_point_head.layers.0.weight": 131072,
  "edpose_model.transformer.decoder.ref_point_head.layers.0.bias": 256,
  "edpose_model.transformer.decoder.ref_point_head.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.ref_point_head.layers.1.bias": 256,
  "edpose_model.transformer.decoder.hw.weight": 34,
  "edpose_model.transformer.decoder.keypoint_embed.weight": 4352,
  "edpose_model.transformer.decoder.gesture_embed.weight": 25600,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.0.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.1.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.2.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.3.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.4.layers.2.bias": 4,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.0.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.1.bias": 256,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.2.weight": 1024,
  "edpose_model.transformer.decoder.bbox_embed.5.layers.2.bias": 4,
  "edpose_model.transformer.decoder.pose_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.0.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.0.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.1.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.1.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.1.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.1.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.1.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.1.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.2.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.2.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.2.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.2.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.2.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.2.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.3.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.3.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.3.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.3.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.3.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.3.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_embed.4.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.4.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.4.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_embed.4.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_embed.4.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_embed.4.layers.2.bias": 2,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.0.weight": 65536,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.0.bias": 256,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.1.weight": 65536,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.1.bias": 256,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.2.weight": 512,
  "edpose_model.transformer.decoder.pose_hw_embed.0.layers.2.bias": 2,
  "edpose_model.transformer.decoder.class_embed.0.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.0.bias": 7,
  "edpose_model.transformer.decoder.class_embed.1.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.1.bias": 7,
  "edpose_model.transformer.decoder.class_embed.2.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.2.bias": 7,
  "edpose_model.transformer.decoder.class_embed.3.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.3.bias": 7,
  "edpose_model.transformer.decoder.class_embed.4.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.4.bias": 7,
  "edpose_model.transformer.decoder.class_embed.5.weight": 1792,
  "edpose_model.transformer.decoder.class_embed.5.bias": 7,
  "edpose_model.transformer.enc_output.weight": 65536,
  "edpose_model.transformer.enc_output.bias": 256,
  "edpose_model.transformer.enc_output_norm.weight": 256,
  "edpose_model.transformer.enc_output_norm.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.0.weight": 65536,
  "edpose_model.transformer.enc_out_bbox_embed.layers.0.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.1.weight": 65536,
  "edpose_model.transformer.enc_out_bbox_embed.layers.1.bias": 256,
  "edpose_model.transformer.enc_out_bbox_embed.layers.2.weight": 1024,
  "edpose_model.transformer.enc_out_bbox_embed.layers.2.bias": 4,
  "edpose_model.transformer.enc_out_class_embed.weight": 1792,
  "edpose_model.transformer.enc_out_class_embed.bias": 7,
  "edpose_model.label_enc.weight": 25856,
  "edpose_model.input_proj.0.0.weight": 131072,
  "edpose_model.input_proj.0.0.bias": 256,
  "edpose_model.input_proj.0.1.weight": 256,
  "edpose_model.input_proj.0.1.bias": 256,
  "edpose_model.input_proj.1.0.weight": 262144,
  "edpose_model.input_proj.1.0.bias": 256,
  "edpose_model.input_proj.1.1.weight": 256,
  "edpose_model.input_proj.1.1.bias": 256,
  "edpose_model.input_proj.2.0.weight": 524288,
  "edpose_model.input_proj.2.0.bias": 256,
  "edpose_model.input_proj.2.1.weight": 256,
  "edpose_model.input_proj.2.1.bias": 256,
  "edpose_model.input_proj.3.0.weight": 4718592,
  "edpose_model.input_proj.3.0.bias": 256,
  "edpose_model.input_proj.3.1.weight": 256,
  "edpose_model.input_proj.3.1.bias": 256,
  "edpose_model.backbone.0.body.layer2.0.conv1.weight": 32768,
  "edpose_model.backbone.0.body.layer2.0.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.0.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.0.downsample.0.weight": 131072,
  "edpose_model.backbone.0.body.layer2.1.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.1.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.1.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.2.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.2.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.2.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer2.3.conv1.weight": 65536,
  "edpose_model.backbone.0.body.layer2.3.conv2.weight": 147456,
  "edpose_model.backbone.0.body.layer2.3.conv3.weight": 65536,
  "edpose_model.backbone.0.body.layer3.0.conv1.weight": 131072,
  "edpose_model.backbone.0.body.layer3.0.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.0.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.0.downsample.0.weight": 524288,
  "edpose_model.backbone.0.body.layer3.1.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.1.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.1.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.2.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.2.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.2.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.3.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.3.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.3.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.4.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.4.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.4.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer3.5.conv1.weight": 262144,
  "edpose_model.backbone.0.body.layer3.5.conv2.weight": 589824,
  "edpose_model.backbone.0.body.layer3.5.conv3.weight": 262144,
  "edpose_model.backbone.0.body.layer4.0.conv1.weight": 524288,
  "edpose_model.backbone.0.body.layer4.0.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.0.conv3.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.0.downsample.0.weight": 2097152,
  "edpose_model.backbone.0.body.layer4.1.conv1.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.1.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.1.conv3.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.2.conv1.weight": 1048576,
  "edpose_model.backbone.0.body.layer4.2.conv2.weight": 2359296,
  "edpose_model.backbone.0.body.layer4.2.conv3.weight": 1048576,
  "decoder.layers.0.cross_attn.sampling_offsets.weight": 65536,
  "decoder.layers.0.cross_attn.sampling_offsets.bias": 256,
  "decoder.layers.0.cross_attn.attention_weights.weight": 32768,
  "decoder.layers.0.cross_attn.attention_weights.bias": 128,
  "decoder.layers.0.cross_attn.value_proj.weight": 65536,
  "decoder.layers.0.cross_attn.value_proj.bias": 256,
  "decoder.layers.0.cross_attn.output_proj.weight": 65536,
  "decoder.layers.0.cross_attn.output_proj.bias": 256,
  "decoder.layers.0.norm1.weight": 256,
  "decoder.layers.0.norm1.bias": 256,
  "decoder.layers.0.self_attn.in_proj_weight": 196608,
  "decoder.layers.0.self_attn.in_proj_bias": 768,
  "decoder.layers.0.self_attn.out_proj.weight": 65536,
  "decoder.layers.0.self_attn.out_proj.bias": 256,
  "decoder.layers.0.norm2.weight": 256,
  "decoder.layers.0.norm2.bias": 256,
  "decoder.layers.0.linear1.weight": 524288,
  "decoder.layers.0.linear1.bias": 2048,
  "decoder.layers.0.linear2.weight": 524288,
  "decoder.layers.0.linear2.bias": 256,
  "decoder.layers.0.norm3.weight": 256,
  "decoder.layers.0.norm3.bias": 256,
  "decoder.layers.1.cross_attn.sampling_offsets.weight": 65536,
  "decoder.layers.1.cross_attn.sampling_offsets.bias": 256,
  "decoder.layers.1.cross_attn.attention_weights.weight": 32768,
  "decoder.layers.1.cross_attn.attention_weights.bias": 128,
  "decoder.layers.1.cross_attn.value_proj.weight": 65536,
  "decoder.layers.1.cross_attn.value_proj.bias": 256,
  "decoder.layers.1.cross_attn.output_proj.weight": 65536,
  "decoder.layers.1.cross_attn.output_proj.bias": 256,
  "decoder.layers.1.norm1.weight": 256,
  "decoder.layers.1.norm1.bias": 256,
  "decoder.layers.1.self_attn.in_proj_weight": 196608,
  "decoder.layers.1.self_attn.in_proj_bias": 768,
  "decoder.layers.1.self_attn.out_proj.weight": 65536,
  "decoder.layers.1.self_attn.out_proj.bias": 256,
  "decoder.layers.1.norm2.weight": 256,
  "decoder.layers.1.norm2.bias": 256,
  "decoder.layers.1.linear1.weight": 524288,
  "decoder.layers.1.linear1.bias": 2048,
  "decoder.layers.1.linear2.weight": 524288,
  "decoder.layers.1.linear2.bias": 256,
  "decoder.layers.1.norm3.weight": 256,
  "decoder.layers.1.norm3.bias": 256,
  "decoder.norm.weight": 256,
  "decoder.norm.bias": 256,
  "decoder.ref_point_head.layers.0.weight": 131072,
  "decoder.ref_point_head.layers.0.bias": 256,
  "decoder.ref_point_head.layers.1.weight": 65536,
  "decoder.ref_point_head.layers.1.bias": 256,
  "decoder.bbox_embed.0.layers.0.weight": 65536,
  "decoder.bbox_embed.0.layers.0.bias": 256,
  "decoder.bbox_embed.0.layers.1.weight": 65536,
  "decoder.bbox_embed.0.layers.1.bias": 256,
  "decoder.bbox_embed.0.layers.2.weight": 1024,
  "decoder.bbox_embed.0.layers.2.bias": 4,
  "decoder.bbox_embed.1.layers.0.weight": 65536,
  "decoder.bbox_embed.1.layers.0.bias": 256,
  "decoder.bbox_embed.1.layers.1.weight": 65536,
  "decoder.bbox_embed.1.layers.1.bias": 256,
  "decoder.bbox_embed.1.layers.2.weight": 1024,
  "decoder.bbox_embed.1.layers.2.bias": 4,
  "class_embed.weight": 1792,
  "class_embed.bias": 7,
  "ref_transform.weight": 288,
  "ref_transform.bias": 4,
  "query_transform.weight": 1179648,
  "query_transform.bias": 256
}
